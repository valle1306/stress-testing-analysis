{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE-UrC46YPe8"
   },
   "source": [
    "# MSDS 534: Statistical Learning - Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYGBa1xOYPe9"
   },
   "source": [
    "### NAME:  Phan Nguyen Huong Le\n",
    "\n",
    "### NET ID:  hpl14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6YV1_BEYPe-"
   },
   "source": [
    "In this homework, we predict whether a recipe is vegetarian or not vegetarian, given the description and ingredients. The data is from https://www.kaggle.com/datasets/shuyangli94/foodcom-recipes-with-search-terms-and-tags. The data has been processed to retain mostly dinner recipes, and sampled to ensure 50/50 vegetarian/non-vegetarian recipes.\n",
    "\n",
    "In particular, we compare:\n",
    "1.  a bag-of-words version, where the input data is a matrix of `recipes x n_ingredients`. Row `i` has a `1` in column `j` if recipe `i` uses ingredient `j`.\n",
    "2. a DistilBERT version, where the input data is a matrix of `recipes x bert_input`, where `bert_input` is a string containing the recipe title, description and ingredient list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h7mZodRYPe-"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "import itertools\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4Nv1RGJYPe_"
   },
   "source": [
    "### BERT data\n",
    "\n",
    "- `recipes_processed.csv` is a matrix where each row is a recipe. The column `bert_input` are going to be the input to the DistilBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1W0kpyPIYPe_",
    "outputId": "da2edb97-c412-4365-a4de-c03417481240"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          bert_input  vegetarian\n",
      "0  Creamy Chicken &amp; Pasta. Prepare this ahead...       False\n",
      "1  Vidalia Onion Dip. This recipe was my grandmot...        True\n",
      "2  Veggie Lasagna. From Simple and Delicious.  I ...        True\n",
      "3  Restaurant Plaisir/Sante Ratatouille. This sim...        True\n",
      "4  Fresh Corn Chowder. Naturally sweet corn and c...        True\n",
      "(22000, 2)\n"
     ]
    }
   ],
   "source": [
    "bert_df = pd.read_csv('recipes_processed.csv')\n",
    "\n",
    "bert_df = bert_df[[\"bert_input\", \"vegetarian\"]]\n",
    "\n",
    "print(bert_df.head())\n",
    "print(bert_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogZBTObCYPe_"
   },
   "source": [
    "We import the DistilBERT tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBCjEr_TYPe_"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "mybert = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWB_cPBjYPe_"
   },
   "source": [
    "### Bag-of-words data\n",
    "\n",
    "- 'recipes_bow.csv' is the `recipes x vocab_size` matrix, where `vocab_size` is the total number of ingredients\n",
    "- 'ingredient_dict.csv' is the lookup table for the vocabulary of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuTTWoKPYPe_"
   },
   "outputs": [],
   "source": [
    "bow_df = pd.read_pickle('recipes_bow.pkl')\n",
    "ingredient_dict = pd.read_csv('ingredient_dict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARtdJNKhYPfA",
    "outputId": "284b4090-5813-4567-b629-a6e562c32153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ingred_0  ingred_1  ingred_2  ingred_3\n",
      "0         0         0         0         0\n",
      "1         0         0         0         0\n",
      "2         0         0         0         0\n",
      "3         0         1         0         1\n",
      "4         1         0         0         0\n",
      "     ingred          vocab\n",
      "0  ingred_0           salt\n",
      "1  ingred_1      olive oil\n",
      "2  ingred_2          onion\n",
      "3  ingred_3  garlic cloves\n",
      "4  ingred_4          water\n",
      "(22000, 7080)\n"
     ]
    }
   ],
   "source": [
    "print(bow_df.iloc[:, range(4)].head())\n",
    "print(ingredient_dict.head())\n",
    "print(bow_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF0FEZgUYPfA"
   },
   "outputs": [],
   "source": [
    "n_vocab = ingredient_dict.shape[0]\n",
    "bow_feats = ['ingred_'+str(j) for j in range(n_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WV0QcR9jYPfA"
   },
   "outputs": [],
   "source": [
    "xbow = np.array(bow_df[bow_feats])\n",
    "ybow = np.array(bow_df['vegetarian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoWV2HoBYPfA"
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "(a) Turn the bag-of-words data into torch tensors.\n",
    "\n",
    "(b) Turn the strings `bert_input` into tokens for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AkrDxF9YPfA"
   },
   "outputs": [],
   "source": [
    "# turn bow data into torch tensors (hint: make sure they are of torch.float type)\n",
    "\n",
    "xbow = torch.tensor(xbow, dtype = torch.float32)\n",
    "ybow = torch.tensor(ybow, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uGFxoc6mYPfA"
   },
   "outputs": [],
   "source": [
    "# Turn bert_df['bert_input'] into a list (hint: pandas has an easy function to turn a column into a list)\n",
    "\n",
    "bert_list = bert_df['bert_input'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgsVSc-KYPfA"
   },
   "outputs": [],
   "source": [
    "# turn bert_list into tokens using tokenizer with max_length=180, padding='max_length', truncation=True\n",
    "bert_tokens = tokenizer(bert_list, max_length=180, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKhy0Fg6YPfA"
   },
   "outputs": [],
   "source": [
    "# get both the input_ids and the attention mask from bert_tokens\n",
    "bert_ids = bert_tokens['input_ids']\n",
    "bert_mask = bert_tokens['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pyFWQSJYPfA"
   },
   "outputs": [],
   "source": [
    "# turn bert_df['vegetarian'] into a torch tensor.\n",
    "ybert = torch.tensor(bert_df['vegetarian'], dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06-I-_IqYPfA"
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "Set up a torch dataset called BERT Dataset. The purpose of this dataset is to have an easy way to access:\n",
    "- input_ids\n",
    "- attention_mask\n",
    "- ybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg1zmz-EYPfB"
   },
   "outputs": [],
   "source": [
    "class BERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, label):\n",
    "\n",
    "        #self.input_ids = torch.tensor(input_ids, dtype = torch.long)\n",
    "        #self.attention_mask = torch.tensor(attention_mask, dtype = torch.float32)\n",
    "        #self.label = torch.tensor(label, dtype = torch.float32)\n",
    "\n",
    "        self.input_ids = input_ids.dataset\n",
    "        self.attention_mask = attention_mask.dataset\n",
    "        self.label = label.dataset\n",
    "        self.indices = input_ids.indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        actual_idx = self.indices[idx]\n",
    "        input_ids = self.tokens['input_ids'][idx]\n",
    "        attention_mask = self.tokens['attention_mask'][idx]\n",
    "        label = self.label[idx]\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvrIEFK7YPfB"
   },
   "source": [
    "### Question 3:\n",
    "\n",
    "Split both BOW data and BERT data into training and test datasets. You should use the same indices for both datasets (i.e. training examples in BOW are the same training examples in BERT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5dg4H7aYPfB"
   },
   "outputs": [],
   "source": [
    "n = BERTDataset(bert_ids, bert_mask, ybert)\n",
    "split = np.random.binomial(1, p=0.9, size=len(n)) # 1 if training, 0 if test\n",
    "train_ind = int(0.5*len(n))\n",
    "test_ind = len(n) - train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_tkmixWYPfB"
   },
   "outputs": [],
   "source": [
    "xbow_train = torch.utils.data.Subset(xbow, train_ind)\n",
    "xbow_test = torch.utils.data.Subset(xbow, test_ind)\n",
    "\n",
    "ybow_train = torch.utils.data.Subset(ybow, train_ind)\n",
    "ybow_test = torch.utils.data.Subset(ybow, test_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xidBYu13YPfB"
   },
   "outputs": [],
   "source": [
    "# bert input ids\n",
    "bert_ids_train = torch.utils.data.Subset(bert_ids, train_ind)\n",
    "bert_ids_test = torch.utils.data.Subset(bert_ids, test_ind)\n",
    "\n",
    "# bert attention mask\n",
    "bert_mask_train = torch.utils.data.Subset(bert_mask, train_ind)\n",
    "bert_mask_test = torch.utils.data.Subset(bert_mask, test_ind)\n",
    "\n",
    "# bert label\n",
    "ybert_train = torch.utils.data.Subset(ybert, train_ind)\n",
    "ybert_test = torch.utils.data.Subset(ybert, test_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvScVz0uYPfB"
   },
   "source": [
    "### Question 4:\n",
    "\n",
    "(a) Create a neural network with 2 hidden layers for the BOW data to predict vegetarian/non-vegetarian.\n",
    "\n",
    "(b) Create a neural network which uses a single layer on top of the distilBERT model to predict vegetarian/non-vegetarian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMEQqkjwYPfB"
   },
   "outputs": [],
   "source": [
    "## neural network model\n",
    "class BOWNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        super(BOWNet, self).__init__()\n",
    "\n",
    "        # hidden_dims is a 2-dim list: hidden_dims[0], hidden_dims[1]\n",
    "        # initialize a neural network with linear, ReLU, linear, dropout(p=0.5), linear, ReLU, linear\n",
    "        # what should the final function be after the last linear layer? (hint: y is {0, 1})\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dims[1], 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y_pred = self.layers(x).flatten()\n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "\n",
    "        # y is binary {0, 1}. what should the loss function be?\n",
    "        loss_fn = nn.BCELoss()\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KHcE64XYPfB"
   },
   "outputs": [],
   "source": [
    "class BERTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTNet, self).__init__()\n",
    "\n",
    "        self.bert = mybert # this is the distilbert model from the beginning of the notebook\n",
    "        self.layer1 = nn.Sequential(nn.Linear(self.bert.config.hidden_size, 1),\n",
    "                                    nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_outputs[0][:,0,:] # this should get the <CLS> token from distilBERT\n",
    "\n",
    "        x = self.layer1(pooled_output)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, y_pred, y):\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NGwWRZAYPfB"
   },
   "source": [
    "### Question 5:\n",
    "\n",
    "(a) Create torch datasets and data loaders for the BOW and BERT data.\n",
    "\n",
    "(b) Instantiate a BERT model and a BOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ue-TMj_erGkX",
    "outputId": "537db89b-5ac1-4f30-bef4-3bb57e0cc0ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7941a8d75c00>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ybert_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRHrSFtBzfTo",
    "outputId": "8ec371d8-7e3b-491d-c6a0-56ca8cee4431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.Subset'> <class 'torch.utils.data.dataset.Subset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(xbow_train), type(ybow_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "iookUVvjYPfB",
    "outputId": "d6c0c483-747b-4ab0-f5de-0f40ca3f9a7b"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1f744ec7d4c7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#bow_train = TensorDataset(xbow_train, ybow_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbow_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxbow_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"replacement should be a boolean value, but got replacement={self.replacement}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36mnum_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# dataset size might change at runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "bert_train = BERTDataset(bert_ids_train, bert_mask_train, ybert_train)\n",
    "bert_loader = DataLoader(bert_train, batch_size=10, shuffle=True)\n",
    "\n",
    "#bow_train = TensorDataset(xbow_train, ybow_train)\n",
    "bow_loader = DataLoader(xbow_train, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95kQeHrNYPfC"
   },
   "outputs": [],
   "source": [
    "bert_model = BERTNet(mybert)\n",
    "bow_model = BOWNet(input_dim, hidden_dims=[500,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Tnl7EShYPfC"
   },
   "source": [
    "### Question 6:\n",
    "\n",
    "(a) Train the BERT neural network to predict vegetarian/not vegetarian.\n",
    "\n",
    "Note: each epoch may take a while (on Prof Moran's laptop, each epoch took around 20 mins). You can obtain a batch of data from your data loader to test your code using `next(iter(bert_loader))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVGHeZCwYPfC"
   },
   "outputs": [],
   "source": [
    "# train BERT neural network\n",
    "\n",
    "epochs = 2\n",
    "lr = 1e-4\n",
    "optimizer = optim.Adam([INSERT CODE])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    batch_iter = 0\n",
    "\n",
    "    for batch in bert_loader:\n",
    "\n",
    "        # get predictions\n",
    "        y_pred = bert_model([INPUT CODE])\n",
    "\n",
    "        # calculate loss\n",
    "        loss = [INPUT CODE]\n",
    "\n",
    "        # compute gradients in a backward pass\n",
    "        [INPUT CODE]\n",
    "\n",
    "        # take a gradient step to update parameters\n",
    "        [INPUT CODE]\n",
    "\n",
    "        # zero gradients in preparation for next iteration\n",
    "        [INPUT CODE]\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        batch_iter += 1\n",
    "        if batch_iter % 20 == 0:\n",
    "            print('epoch: ', epoch, 'batch: ', batch_iter)\n",
    "\n",
    "    print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuAm35-tYPfC"
   },
   "outputs": [],
   "source": [
    "## save model parameters\n",
    "torch.save(bert_model.state_dict(), \"recipe-distilbert.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3vQdSyjYPfC"
   },
   "source": [
    "(b) Train the BOW neural network to predict vegetarian or not vegetarian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifRK0ZEnYPfC"
   },
   "outputs": [],
   "source": [
    "## train neural network to predict vegetarian\n",
    "\n",
    "epochs = 100\n",
    "lr = 1e-4\n",
    "optimizer = [INPUT CODE]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for x_batch, y_batch in bow_loader:\n",
    "\n",
    "        [INPUT CODE]\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJUgDPJVYPfC"
   },
   "outputs": [],
   "source": [
    "## save model parameters\n",
    "torch.save(bow_model.state_dict(), \"recipe-bow.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSdfgDr0YPfC"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "(a) Calculate test accuracy for the BERT model. Note: this is done as a for loop as it requires too much memory to input all test data at once into the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F670cXAaYPfC"
   },
   "outputs": [],
   "source": [
    "## calculate testing error\n",
    "## takes about 1.5 minutes to run\n",
    "\n",
    "bert_model.eval()\n",
    "bert_acc = 0\n",
    "\n",
    "n_test = len(test_ind)\n",
    "\n",
    "for i in range(n_test):\n",
    "\n",
    "    # get predictions from bert_ids_test, bert_mask_test (note: bert_model expects dimensions batch_size x max_length)\n",
    "    # that is, make sure inputs are 2-dim torch tensors (1 x max_length)\n",
    "    pred = [INPUT CODE]\n",
    "\n",
    "    # turn probability into {0, 1}\n",
    "    pred = [INPUT CODE]\n",
    "\n",
    "    bert_acc = bert_acc + (pred == ybert_test[i]).sum() / len(pred)\n",
    "\n",
    "bert_acc = bert_acc / n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g27HvDsYPfG"
   },
   "source": [
    "(b) Calculate test accuracy for the BOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2H30_FXYPfG"
   },
   "outputs": [],
   "source": [
    "ybow_pred = bow_model([INPUT CODE])\n",
    "bow_acc = [INPUT CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBcdHQVPYPfG"
   },
   "outputs": [],
   "source": [
    "print(\"BERT accuracy:\", bert_acc, 'BOW accuracy:', bow_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C71Yb2TYPfG"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "How do the BERT and BOW accuracies compare? Comment on the differences between the BERT and BOW models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHyWR5FUYPfG"
   },
   "source": [
    "[Answer]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx0pnob1YPfG"
   },
   "source": [
    "Just for fun: input your own recipe ingredients and see what the BERT model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6xThM5IYPfG"
   },
   "outputs": [],
   "source": [
    "my_recipe_collection = [\n",
    "\"The ingredients are: tofu, bread, arugula, nutritional yeast\",\n",
    "\"Recipe uses: beef, milk, eggs, ketchup, worchestershire sauce\"\n",
    "]\n",
    "\n",
    "for recipe in my_recipe_collection:\n",
    "\n",
    "    recipe_tokens = tokenizer(recipe, return_tensors='pt',\n",
    "                                max_length=180, padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "    my_pred = bert_model(recipe_tokens['input_ids'], recipe_tokens['attention_mask'])\n",
    "    my_pred = my_pred.detach().numpy()\n",
    "\n",
    "    print(my_pred)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
