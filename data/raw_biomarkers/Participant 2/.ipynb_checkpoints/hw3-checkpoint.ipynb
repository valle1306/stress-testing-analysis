{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSDS 534: Statistical Learning - Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAME:  Phan Nguyen Huong Le\n",
    "\n",
    "### NET ID:  hpl14\n",
    "\n",
    "### Teammates: __________ (write \"Not Applicable\" if you worked alone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we analyze the [dsprites dataset](https://github.com/google-deepmind/dsprites-dataset) using a variational autoencoder with convolutional layers.\n",
    "\n",
    "The `dSprites` dataset (Matthey et al. 2017) was created as a benchmark dataset to evaluate the \"disentanglement\" capabilities of VAEs. That is, how well can VAEs \"disentangle\" latent factors which generated the data? \n",
    "\n",
    "`dSprites` is a dataset of 2D shapes procedurally generated from 6 ground truth independent latent factors. These factors are color, shape, scale, rotation, x and y positions of a sprite.\n",
    "\n",
    "All possible combinations of these latents are present exactly once, generating $737280$ total images. \n",
    "\n",
    "We will analyze a subset of this data of size $N=60000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Q0: Download `dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz` from Canvas/Files and save in a folder called `data` in the working directory of this notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to load the data\n",
    "def load_dsprites(n_samples=n_samples, seed=42):\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\tdataset_zip = np.load('data/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz')\n",
    "\n",
    "\timgs = dataset_zip['imgs']\n",
    "\tlatents_values = dataset_zip['latents_values']\n",
    "\tlatents_classes = dataset_zip['latents_classes']\n",
    "\n",
    "\t# subsample data\n",
    "\tind = np.random.choice(range(imgs.shape[0]), n_samples, replace=False)\n",
    "\timgs = imgs[ind, :, :]\n",
    "\tlatents_values = latents_values[ind, :]\n",
    "\tlatents_classes = latents_classes[ind, :]\n",
    "\n",
    "\t# add channel dimension\n",
    "\timgs = imgs[:, None, :, :]\n",
    "\n",
    "\t# remove first column, it's all zeros\n",
    "\tlatents_classes = latents_classes[:, 1:latents_classes.shape[1]]\n",
    "\n",
    "\treturn imgs, latents_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, latents = load_dsprites()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Q1: Complete the following code to get the input and latent dimensions for your VAE. Note that the input dimension should be the height of the image (NOT height * width)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = [INSERT CODE] # height of image\n",
    "latent_dim = [INSERT CODE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example figures\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    train_image = images[i]\n",
    "    plt.imshow(train_image[0], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Q2: Create a `TensorDataset` and a `DataLoader` (with `batch_size=100`) for the data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "torch_images = torch.tensor(images, dtype=torch.float)\n",
    "torch_latents = torch.tensor(latents, dtype=torch.float)\n",
    "dataset = [INSERT CODE]\n",
    "train_loader = [INSERT CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Q3: Fill in the following code to build a VAE with convolutional layers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(torch.nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, loss_type='mse'):\n",
    "        super(ConvVAE, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        kernel_size=4\n",
    "        stride=2\n",
    "        padding=1\n",
    "\n",
    "        channels_encoder = [[1, 32],\n",
    "                            [32, 16]]\n",
    "\n",
    "        self.cnn_encode = nn.Sequential(nn.Conv2d([INSERT CODE], [INSERT CODE], kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d([INSERT CODE], [INSERT CODE], kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "                                        nn.ReLU())\n",
    "        \n",
    "        # calculate the height dimension of the output of cnn_encode\n",
    "        self.out_dim = [INSERT CODE]\n",
    "\n",
    "        # specify the channel dimension of the output of cnn_encode\n",
    "        out_channel = [INSERT CODE]\n",
    "\n",
    "        # calculate the dimension of the flattened convolutional output i.e. channel x height x width\n",
    "        fc_input_dim = [INSERT CODE]\n",
    "        self.fc_encode = nn.Linear(fc_input_dim, 256)\n",
    "\n",
    "        self.z_mean = nn.Linear(256, latent_dim)\n",
    "        self.z_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "        self.fc_decode = nn.Sequential(nn.Linear(latent_dim, 256),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(256, [INSERT CODE]), \n",
    "                                     nn.ReLU())\n",
    "\n",
    "        self.cnn_decode = nn.Sequential(nn.ConvTranspose2d([INSERT CODE], [INSERT CODE], kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.ConvTranspose2d([INSERT CODE], [INSERT CODE], kernel_size=kernel_size, stride=stride, padding=padding))\n",
    "                                        \n",
    "\n",
    "    def encode(self, x):\n",
    "\n",
    "        [INSERT CODE]\n",
    "\n",
    "        return z_mean, z_log_var\n",
    "\n",
    "    def reparameterize(self, mean, log_var):\n",
    "        \n",
    "        [INSERT CODE]\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def decode(self, z):\n",
    "\n",
    "        [INSERT CODE]\n",
    "\n",
    "        if self.loss_type == 'binary':\n",
    "            [INSERT CODE]\n",
    "\n",
    "        return x_reconstructed\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encode(x)\n",
    "        z = self.reparameterize(z_mean, z_log_var)\n",
    "        x_mean = self.decode(z)\n",
    "\n",
    "        return x_mean, z, z_mean, z_log_var\n",
    "\n",
    "    def reconstruction_loss(self, x_pred, x):\n",
    "\n",
    "        # flatten images\n",
    "        x_pred = x_pred.reshape(x_pred.shape[0], -1)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        if self.loss_type == 'mse':\n",
    "            [INSERT CODE]\n",
    "\n",
    "        if self.loss_type == 'binary':\n",
    "            [INSERT CODE]\n",
    "\n",
    "        return reconstruction_loss\n",
    "\n",
    "    def vae_loss(self, x):\n",
    "\n",
    "        x_mean, z, z_mean, z_log_var = self.forward(x)\n",
    "\n",
    "        reconstruction_loss = self.reconstruction_loss(x_mean, x)\n",
    "\n",
    "        kl_loss = [INSERT CODE]\n",
    "\n",
    "\n",
    "        return reconstruction_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = ConvVAE(input_dim, latent_dim, loss_type='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Q4: Train the `ConvVAE`. (Note this may take an hour)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "epochs = 40\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_recon = 0\n",
    "    epoch_kl = 0\n",
    "\n",
    "    for [INSERT CODE]\n",
    "\n",
    "        reconstruction_loss, kl_loss = [INSERT CODE]\n",
    "        total_loss = [INSERT CODE]\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_recon += reconstruction_loss.item()\n",
    "        epoch_kl += kl_loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "    epoch_recon = epoch_recon / len(train_loader)\n",
    "    epoch_kl = epoch_kl / len(train_loader)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\", 'recon loss:', f\"{epoch_recon:.3}\",\n",
    "              'kld:', f\"{epoch_kl:.3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Q5: Fill in the following code to plot reconstructions of the images._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "n_img = 10\n",
    "num_cols = 5\n",
    "num_rows = 2\n",
    "plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
    "\n",
    "inds = np.random.choice(range(n_samples), size=n_img)\n",
    "\n",
    "for i in range(n_img):\n",
    "\n",
    "  ind = inds[i]\n",
    "\n",
    "  x = images[ind]\n",
    "  x = x[None, :, :, :] # add N dimension\n",
    "  x = torch.tensor(x, dtype=torch.float)\n",
    "\n",
    "  # get reconstruction of image x from ConvVAE\n",
    "  x_recon = [INSERT CODE]\n",
    "\n",
    "  plt.subplot(num_rows, 2 * num_cols, i + 1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(x[0][0].detach().cpu().numpy(), cmap=plt.cm.binary)\n",
    "\n",
    "  plt.subplot(num_rows, 2 * num_cols, (i + 1) + 2 * num_cols)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(x_recon.detach().cpu().numpy(), cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Q6: Create latent traversal plots to visualize how changing the first 2 dimensions of the latent space affects the output images._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of latent vectors for traversal\n",
    "n_samples = 20\n",
    "grid_x = np.linspace(-3, 3, n_samples)\n",
    "grid_y = np.linspace(-3, 3, n_samples)\n",
    "traversal_grid = torch.FloatTensor(n_samples, n_samples, [INSERT CODE])\n",
    "\n",
    "for i, xi in enumerate(grid_x):\n",
    "    for j, yi in enumerate(grid_y):\n",
    "        z_sample = torch.FloatTensor([INSERT CODE])\n",
    "        traversal_grid[j, i] = z_sample\n",
    "\n",
    "# Generate images from the latent traversal grid\n",
    "with torch.no_grad():\n",
    "    traversal_grid = traversal_grid.view(-1, latent_dim)\n",
    "    generated_images = [INSERT CODE]\n",
    "\n",
    "# Plot the generated images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(n_samples):\n",
    "    for j in range(n_samples):\n",
    "        plt.subplot(n_samples, n_samples, i * n_samples + j + 1)\n",
    "        plt.imshow(generated_images[i * n_samples + j].squeeze().cpu().numpy(), cmap='gray')\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
